<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>motion-planning on Balakumar Sundaralingam</title>
    <link>/tags/motion-planning/</link>
    <description>Recent content in motion-planning on Balakumar Sundaralingam</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Balakumar Sundaralingam</copyright>
    <lastBuildDate>Mon, 01 Oct 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/tags/motion-planning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Robot Perception</title>
      <link>/project/robot_perception/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/robot_perception/</guid>
      <description>This project aims to explore perception to aid robot manipulation. Closed loop feedback control for manipulation tasks have been lacking in the real world due to limitations of current perception approaches. Could perception from different sources such as tactile, vision and aural fill in the gaps existing in a single source? As a first step, we worked on building a robust mapping from raw tactile signals to force. This work leveraged neural networks for learning the mapping in a supervised setup.</description>
    </item>
    
    <item>
      <title>Dexterous Manipulation</title>
      <link>/project/dexterous_manipulation/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/dexterous_manipulation/</guid>
      <description>Advisor: Prof. Tucker Hermans Robots with multi-fingered hands and human-like tactile sensing allow for complex interactions with the environmnet. Consider a robot replacing a lightbulb, using a drill or pouring from a mustard bottle. These tasks while trivial for humans, remain challenging for robots. Current approaches to dexterous manipulation with robotic systems either require complete analytic models of the environment or extensive interaction with the object to learn a policy.</description>
    </item>
    
    <item>
      <title>Artificial Potential Field Collision Avoidance for holonomic mobile robots</title>
      <link>/post/collision_avoidance/</link>
      <pubDate>Fri, 01 May 2015 00:00:00 -0600</pubDate>
      
      <guid>/post/collision_avoidance/</guid>
      <description>Contributors: Balakumar Sundaralingam, Rohith Prasad.
Introduction The use of tele-operated robots in search and rescue, environment monitoring and interstellar exploration has increased in the past years. Tele-operation of robots is a complex task. The operator not only has to focus on the task but also perform collision avoidance so that the robot does not collide with obstacles in the environment. The addition of an automatic collision avoidance system to the robot will ease the operator and allow the operator to focus more on the task.</description>
    </item>
    
    <item>
      <title>Optic Flow based  Collision Avoidance for holonomic mobile robots</title>
      <link>/post/optic_flow/</link>
      <pubDate>Fri, 01 May 2015 00:00:00 -0600</pubDate>
      
      <guid>/post/optic_flow/</guid>
      <description>Contributors: Balakumar Sundaralingam.
Introduction Optical flow is one of the many ways to detect motion in a video stream. Conversely it can be used to interact with the real 3d world. Exploring this possibility motivated me to see if optical flow from a single camera is good for collision avoidance. The base idea is to have the environment static and the camera moving. This is discussed in the below sections.</description>
    </item>
    
  </channel>
</rss>