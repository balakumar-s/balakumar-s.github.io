<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Balakumar Sundaralingam on Balakumar Sundaralingam</title>
    <link>/</link>
    <description>Recent content in Balakumar Sundaralingam on Balakumar Sundaralingam</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Balakumar Sundaralingam</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0600</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Planning Multi-Fingered Grasps as Probabilistic Inference in a Learned Deep Network</title>
      <link>/publication/grasp_inference/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/grasp_inference/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Relaxed-Rigidity Constraints: In-Grasp Manipulation using Purely Kinematic Trajectory Optimization</title>
      <link>/publication/in_grasp_manipulation/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/in_grasp_manipulation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>contact</title>
      <link>/contact/</link>
      <pubDate>Sun, 21 May 2017 14:00:48 -0600</pubDate>
      
      <guid>/contact/</guid>
      <description>&lt;p&gt;coming soon.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>about</title>
      <link>/about/</link>
      <pubDate>Sun, 21 May 2017 14:00:43 -0600</pubDate>
      
      <guid>/about/</guid>
      <description>&lt;p&gt;I am a Robotics PhD candidate at the University of Utah. More information &lt;a href=&#34;https://sites.google.com/site/balakumarprofile/home&#34; target=&#34;_blank&#34;&gt;here.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hybrid/Parallel Force/Position Controllers</title>
      <link>/post/force_position_control/</link>
      <pubDate>Sun, 01 May 2016 00:00:00 -0600</pubDate>
      
      <guid>/post/force_position_control/</guid>
      <description>

&lt;h2 id=&#34;hybrid-controllers&#34;&gt;Hybrid controllers:&lt;/h2&gt;

&lt;p&gt;Hybrid and Parallel force/Position controllers was implemented on a two-link manipulator with strain gauges for force sensing. Effect of friction on the controllers was tested with different surfaces.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/W4jrnlV-JEM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34; title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mobile robot simulation in Gazebo</title>
      <link>/post/mobile_robot_simulation/</link>
      <pubDate>Sun, 01 May 2016 00:00:00 -0600</pubDate>
      
      <guid>/post/mobile_robot_simulation/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://github.com/balakumar-s/p3atGazeboRos&#34; target=&#34;_blank&#34;&gt;Git repo&lt;/a&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/z9PzmQjPOmY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34; title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;
&lt;/p&gt;

&lt;p&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/0TZ5NcKv7ZA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34; title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dexterous Manipulation</title>
      <link>/project/dexterous_manipulation/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/dexterous_manipulation/</guid>
      <description>&lt;p&gt;Environment interaction is more than picking and placing objects. Robots with multi-fingered hands and human-like tactile sensing allow for complex interactions with the environmnet. Imagine a robot replacing a lightbulb or using a drill. These tasks while trivial for humans, remain challenging for robots. Current approaches to dexterous manipulation with robotic systems either require complete analytic models of the environment or extensive interaction with the object to learn a policy. This project has two main research goals:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Exploring the limitations of qualitative physics based dexterous manipulation&lt;/li&gt;
&lt;li&gt;Using learning to overcome these limitations.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I am actively exploring this research problem at the &lt;a href=&#34;https://robot-learning.cs.utah.edu/&#34; target=&#34;_blank&#34;&gt;LL4MA lab&lt;/a&gt;, advised by &lt;a href=&#34;https://www.cs.utah.edu/~thermans&#34; target=&#34;_blank&#34;&gt;Dr. Tucker Hermans&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;As part of the first goal, we explored performing in-grasp manipulation with a multi-fingered robotic hand. In-grasp manipulation as the problem of moving an object with reference to the palm from an initial pose to a goal pose without breaking or making contacts. We formulated a trajectory optimization problem with cost terms to perform in-grasp manipulation with constraints on the robot&amp;rsquo;s joint limits. The cost terms attempt to move the object to the desired pose while trying to maintain the initial grasp. Our approach being purely kinematic allowed for performing in-grasp manipulation of novel objects.&lt;/p&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/Gn-yMRjbmPE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34; title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;


&lt;p&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Local Reactive Collision Avoidance for holonomic mobile robots</title>
      <link>/post/collision_avoidance/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 -0600</pubDate>
      
      <guid>/post/collision_avoidance/</guid>
      <description>

&lt;h3 id=&#34;artificial-potential-fields&#34;&gt;Artificial Potential Fields&lt;/h3&gt;

&lt;p&gt;Implemented artificial potential field method of collision avoidance on a Holonomic mobile robot with a LIDAR for sensing. Explored methods to counter the drawbacks of potential field especially the narrow corridor oscillation effect. Simulation of the robot and environment was done in Vrep and the method was also tested on a real robot.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/TYOYem2gdg0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34; title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;
&lt;/p&gt;

&lt;h3 id=&#34;optic-flow&#34;&gt;Optic Flow&lt;/h3&gt;

&lt;p&gt;Optic flow as a method to detect objects closer to the camera was investigated as a method of collision avoidance on a Holonomic mobile robot.With the environment static, the robot was mobile and closer objects tend to have larger optic flow and this was used to avoid obstacles. SIFT was used to detect good features in the environment with a wide angle camera.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/MvWAp7sOZlA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34; title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;
&lt;/p&gt;

&lt;h3 id=&#34;vector-polar-histogram&#34;&gt;Vector Polar Histogram&lt;/h3&gt;


&lt;div style=&#34;position: relative; padding-bottom: 56.25%; padding-top: 30px; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;//www.youtube.com/embed/Z53nglbssII&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%;&#34; allowfullscreen frameborder=&#34;0&#34; title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
 &lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Using Push locations to Classify and predict Object Dynamics</title>
      <link>/post/ml_proj/</link>
      <pubDate>Wed, 20 Apr 2016 00:00:00 -0600</pubDate>
      
      <guid>/post/ml_proj/</guid>
      <description>&lt;p&gt;The goal of this project was to build a classifier to predict the next state of an object given the current state and the input using linear classifiers such as SVM, AMP(Aggressive Margin Perceptron). The object&amp;rsquo;s pose was tracked using BLORT and the input was a point contact which was tracked using BLOB tracking with pointcloud data from a kinect, a dataset was collected for different contact points on the object.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> Mapping using only laser range finder on Mobile Robot</title>
      <link>/project/mobile_robot_mapping/</link>
      <pubDate>Sat, 27 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>/project/mobile_robot_mapping/</guid>
      <description>

&lt;h4 id=&#34;contributors&#34;&gt;Contributors:&lt;/h4&gt;

&lt;p&gt;Balakumar Sundaralingam, Sudarsan Balaji, Yaswanth Kodavali.&lt;/p&gt;

&lt;h4 id=&#34;advisor&#34;&gt;Advisor:&lt;/h4&gt;

&lt;p&gt;Prof. Prem S.&lt;/p&gt;

&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;

&lt;p&gt;Mobile robots employ wheel encoders to generate odometric readings. But the odometric readings from the wheel encoders of the mobile robots are generally erroneous, even in indoor environments. Without proper guiding tools like GPS, it is very hard to localise the robot in each consecutive scan for mapping. To overcome this disadvantage, a generic approach to formulate an error function using wheel encoder readings is followed. This error model could be used in algorithms like EKF, Regression Analysis or Least-Squared Error Reduction, to get a more accurate localisation. This is followed by mapping, necessitating the use of high quality wheel
encoders in the mobile robot for localisation.&lt;/p&gt;

&lt;p&gt;This project aims at eliminating the requirement of such high quality wheel encoders for mapping an indoor environment. This task is achieved with scan data taken at discrete intervals, using line-based or point-based Plot Matching techniques, without the aid of odometric data from the wheel encoders. Currently, mapping of indoor environment has been done using the scan data obtained from the mobile platform. The scan data is used in
a post-processing algorithm which provides promising results. This encourages a better future by completely eliminating the need for wheel encoders, thereby reducing the cost of production, and also the corresponding computational efforts. This project was part of my undergraduate thesis, advised by Prof. Prem .S.&lt;/p&gt;

&lt;h2 id=&#34;problem-definition-and-data-collection&#34;&gt;Problem Definition and Data Collection&lt;/h2&gt;

&lt;p&gt;We define the problem as computing the transformation between consecutive laser scans to enable mapping of the environment. Laser scan data was collected using the Pionerr 3AT robot equipped with a SICK LMS laser range finder. The wheel encoder data is also recorded. The robot is run in &amp;ldquo;Wander mode&amp;rdquo; until it covers all areas of the environment.
&lt;figure&gt;
  &lt;img src=&#34;/img/ug_thesis/data_ac.png&#34; style=&#34;height:200px;&#34;&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;

&lt;p&gt;The laser scan data is converted into the polar coordinates which consists of an angle $\Theta$ and a distance from the sensor $d$. The polar scan matching method brute forces a minimum distane
&lt;figure&gt;
  &lt;img src=&#34;/img/ug_thesis/psm.png&#34; style=&#34;height:500px;&#34;&gt;
  &lt;figcaption&gt; Polar Scan Matching &lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
The orientation has a fixed range~$[0,2\pi]$. The translation distance however is infinitely large. We reduce the translation enumeration by matching the centroids between consecutive scans. When computing the centroid, outliers are removed by thresholding the distance as the outliers are usually data from new exploration.
&lt;figure&gt;
  &lt;img src=&#34;/img/ug_thesis/centroid_1.png&#34; style=&#34;height:250px;&#34;&gt;
  &lt;figcaption&gt; Approximate Centroid Fixing:The green dot shows the position of the
centroid of the detected convex hull of the completed polygon drawn in green. Note that
two similar scans get dissimilar polygons and centroids due to the detection of some new
points in the second scan. Also note that this does not affect the position of the centroid
too much.&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
With polar scan matching, we can thus find homogenous transformations between consecutive scans. This can effectively replace wheel encoders for robot odometry. We then superimpose laser scan data from our collected data with transformations from our approach. This allows us to generate accurate 2D maps of the environment.
&lt;figure&gt;
  &lt;img src=&#34;/img/ug_thesis/ekf_map2.png&#34; style=&#34;height:450px;&#34;&gt;
  &lt;img src=&#34;/img/ug_thesis/cm_map2.png&#34; style=&#34;height:250px;&#34;&gt;
  &lt;figcaption&gt; Map of the mobile robot lab  generated using odometry from EKF on wheel encoders &lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;p style=&#34;text-align:center;&#34;&gt;
&lt;img src=&#34;/img/ug_thesis/ekf_map1.png&#34; style=&#34;display:inline-block; width:250px;&#34;&gt;
&lt;img src=&#34;/img/ug_thesis/cm_map1.png&#34; style=&#34;display:inline-block;  width:250px;&#34;&gt;
&lt;figcaption&gt; A second room with three partitions was also explored&lt;/a&gt;&lt;/figcaption&gt;
&lt;/p&gt;
&lt;/figure&gt;&lt;/p&gt;

&lt;h1 id=&#34;line-extraction&#34;&gt;Line extraction&lt;/h1&gt;

&lt;p&gt;As part of this project,  a line extraction method was formulated to generate lines from laser scan data which is made of points.
&lt;figure&gt;
  &lt;img src=&#34;/img/ug_thesis/sam_approach.png&#34; style=&#34;height:250px;&#34;&gt;
  &lt;figcaption&gt; Split and Merge Algorithm &lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&#34;/img/ug_thesis/sam_plot.png&#34; style=&#34;height:250px;&#34;&gt;
  &lt;figcaption&gt; Split and Merge on Laser Scan data &lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
Split and Merge generates lines with no knowledge of clusters as seen in Fig.2. Nearest neighbor clustering was used to generate clustered lines.
&lt;figure&gt;
  &lt;img src=&#34;/img/ug_thesis/cluster.png&#34; style=&#34;height:250px;&#34;&gt;
  &lt;figcaption&gt; Clustered points are shown in different colors.&lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&#34;/img/ug_thesis/sam_cluster.png&#34; style=&#34;height:250px;&#34;&gt;
  &lt;figcaption&gt; Split and Merge on Clustered Laser Scan data shows better lines &lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
The lines obtained from clustered data produces broken lines. An additional step to line extraction which concatenates lines together based on their slope. The complete line extraction method is given below
&lt;figure&gt;
  &lt;img src=&#34;/img/ug_thesis/line_extraction_chart.png&#34; style=&#34;height:700px;&#34;&gt;
  &lt;figcaption&gt; Improved Line extraction with concatenation of lines based on slope. &lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
  &lt;img src=&#34;/img/ug_thesis/improved_line.png&#34; style=&#34;height:250px;&#34;&gt;
  &lt;figcaption&gt; Improved Line extraction with concatenation of lines based on slope. &lt;/a&gt;&lt;/figcaption&gt;
&lt;/figure&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
