<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on Balakumar Sundaralingam</title>
    <link>/project/</link>
    <description>Recent content in Projects on Balakumar Sundaralingam</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Balakumar Sundaralingam</copyright>
    <lastBuildDate>Mon, 01 Oct 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/project/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Robot Perception</title>
      <link>/project/robot_perception/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/robot_perception/</guid>
      <description>This project aims to explore perception to aid robot manipulation. Closed loop feedback control for manipulation tasks have been lacking in the real world due to limitations of current perception approaches. Could perception from different sources such as tactile, vision and aural fill in the gaps existing in a single source? As a first step, we worked on building a robust mapping from raw tactile signals to force. This work leveraged neural networks for learning the mapping in a supervised setup.</description>
    </item>
    
    <item>
      <title>Dexterous Manipulation</title>
      <link>/project/dexterous_manipulation/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/project/dexterous_manipulation/</guid>
      <description>Advisor: Prof. Tucker Hermans Robots with multi-fingered hands and human-like tactile sensing allow for complex interactions with the environmnet. Consider a robot replacing a lightbulb, using a drill or pouring from a mustard bottle. These tasks while trivial for humans, remain challenging for robots. Current approaches to dexterous manipulation with robotic systems either require complete analytic models of the environment or extensive interaction with the object to learn a policy.</description>
    </item>
    
    <item>
      <title> Mapping using only laser range finder on Mobile Robot</title>
      <link>/project/mobile_robot_mapping/</link>
      <pubDate>Sat, 27 Apr 2013 00:00:00 +0000</pubDate>
      
      <guid>/project/mobile_robot_mapping/</guid>
      <description>Contributors: Balakumar Sundaralingam, Sudarsan Balaji, Yaswanth Kodavali.
Advisor: Prof. Prem S. Abstract Mobile robots employ wheel encoders to generate odometric readings. But the odometric readings from the wheel encoders of the mobile robots are generally erroneous, even in indoor environments. Without proper guiding tools like GPS, it is very hard to localise the robot in each consecutive scan for mapping. To overcome this disadvantage, a generic approach to formulate an error function using wheel encoder readings is followed.</description>
    </item>
    
  </channel>
</rss>